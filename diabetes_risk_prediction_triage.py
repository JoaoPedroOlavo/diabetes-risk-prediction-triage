# -*- coding: utf-8 -*-
"""diabetes-risk-prediction-triage.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mVHZV-Pq1m_YR8DzTSUIrxymgdGt8QT4
"""

# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from imblearn.over_sampling import SMOTE
from collections import Counter
from xgboost import XGBClassifier
import lightgbm as lgb

# Loading Dataset
df = pd.read_csv("/data/diabetes_binary_health_indicators_BRFSS2015.csv")

# Checking dimensions
print(f"The dataset has {df.shape[0]} rows and {df.shape[1]} columns")

# Checking basic integrity and types
print("\n--- Dataframe Information ---")
df.info()

# Data sample
display(df.head())

# Checking basic statistics
display(df.describe().T)

# Checking for duplicate rows
duplicated = df.duplicated().sum()
print(f"There are {duplicated} duplicated rows")

# Checking the imbalance of our target variable
print("\nDistribution of the target variable (Diabetes_binary):")
print(df['Diabetes_binary'].value_counts(normalize=True) * 100)

# Removing duplicated
df = df.drop_duplicates()

# Optimizing data types
cols_to_int = [
    'Diabetes_binary', 'HighBP', 'HighChol', 'CholCheck', 'Smoker', 'Stroke',
    'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump',
    'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'DiffWalk', 'Sex', 'Age', 'Education',
    'Income'
]

# Converting to int8
df[cols_to_int] = df[cols_to_int].astype('int8')

# BMI and the mental/physical health columns can be int16
df[['BMI', 'MentHlth', 'PhysHlth']] = df[['BMI', 'MentHlth', 'PhysHlth']].astype('int16')

print(f"New shape after removing duplicates: {df.shape}")
print(f"Reduced memory usage for: {df.memory_usage().sum() / 1024**2:.2f} MB")

# Style settings
sns.set_theme(style="whitegrid")

# Creating a dashboard with 3 charts
fig, ax = plt.subplots(1, 3, figsize=(20, 6))

#1. Relationship with lifestyle
lifestyle_cols = ['PhysActivity', 'Veggies', 'Fruits']
df_melted = df.melt(id_vars='Diabetes_binary', value_vars=lifestyle_cols)
sns.barplot(data=df_melted, x='variable', y='Diabetes_binary', hue='value', ax=ax[0])
ax[0].set_title('Diabetes vs Lifestyle (0=No, 1=Yes)')
ax[0].set_ylabel('Diabetes Prevalence %')

#2. Relationship with physical health
phys_health_trend = df.groupby("PhysHlth")["Diabetes_binary"].mean()
sns.lineplot(x=phys_health_trend.index, y=phys_health_trend.values, ax=ax[1], marker='o', color='red')
ax[1].set_title('Trend: Bad Health Days vs Diabetes')
ax[1].set_xlabel('Days with Bad Physical Health (0-30)')
ax[1].set_ylabel('Chance of Diabetes')

#3. Relationship with BMI
sns.boxplot(data=df, x="Diabetes_binary", y='BMI', ax=ax[2], palette='Set2')
ax[2].set_title('BMI Distribution by Diabetes Status')

plt.tight_layout()
plt.show()

# Calculating correlation
corr = df.corr()

# Creating a mask to show only the lower triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

plt.figure(figsize=(15, 10))
sns.heatmap(corr, mask=mask, annot=True, fmt=".2f", cmap='coolwarm', center=0)
plt.title("Pearson Correlation Matrix")
plt.show()

# 1. Applying suggested transformations
df_prep = df.copy()
df_prep['BMI'] = df_prep['BMI'].clip(upper=70) # Capping
df_prep['Difficult_Health'] = (df_prep['PhysHlth'] > 15).astype(int) # Your new feature

# 2. Separating Target and Predictors
X = df_prep.drop('Diabetes_binary', axis=1)
y = df_prep['Diabetes_binary']

# 3. Stratified Split (Ensures 14% diabetics in both sets)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… Data prepared and scaled!")

# Creating the model with Class Weights
# n_estimators=100 (100 trees), random_state for reproducibility
weights = {0:1, 1:8}
model_weights = RandomForestClassifier(n_estimators=100, class_weight=weights, random_state=42, n_jobs=-1)

# Training
print("Training the model (this might take a minute)...")
model_weights.fit(X_train_scaled, y_train)

# Predictions
y_pred = model_weights.predict(X_test_scaled)

# Evaluation
print("\n--- Classification Report ---")
print(classification_report(y_test, y_pred))

# Confusion Matrix
ConfusionMatrixDisplay.from_estimator(model_weights, X_test_scaled, y_test, cmap='Blues')
plt.grid(False)
plt.show()

# 1. Creating the SMOTE balancer
sm = SMOTE(random_state=42)

# 2. Generating synthetic data (ONLY on Scaled Training)
X_res, y_res = sm.fit_resample(X_train_scaled, y_train)

print(f"Before SMOTE: {Counter(y_train)}")
print(f"After SMOTE: {Counter(y_res)}")

# 3. Training a new model (this time without weights, as the data is already balanced)
model_smote = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
model_smote.fit(X_res, y_res)

# 4. Prediction and Evaluation (Using the original X_test_scaled!)
y_pred_smote = model_smote.predict(X_test_scaled)

print("\n--- Classification Report with SMOTE ---")
print(classification_report(y_test, y_pred_smote))

# Calculating the ideal weight for XGBoost
# (Total negatives / Total positives)
ratio = (y_train == 0).sum() / (y_train == 1).sum()

# Creating the XGBoost model
xgb_model = XGBClassifier(
    n_estimators=200,
    learning_rate=0.1,
    scale_pos_weight=ratio, # Aggressive automatic balancing
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

xgb_model.fit(X_train_scaled, y_train)

# Instead of just predicting 0 or 1, let's get the PROBABILITY
y_probs = xgb_model.predict_proba(X_test_scaled)[:, 1]

# Applying a Custom Threshold of 0.3 (30%)
# If the chance is > 30%, we'll say it's diabetic
y_pred_custom = (y_probs > 0.3).astype(int)

print("\n--- XGBoost Report with Threshold of 0.3 ---")
print(classification_report(y_test, y_pred_custom))

# Getting feature importances
importances = xgb_model.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plotting
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
plt.title('Which factors does the model use most to detect Diabetes?')
plt.show()

# Creating the LightGBM specific dataset (optional, but recommended)
# The is_unbalance=True parameter acts as the automatic class_weight
lgb_model = lgb.LGBMClassifier(
    n_estimators=500,
    learning_rate=0.05,
    is_unbalance=True, # Handles the 86/14 imbalance
    random_state=42,
    importance_type='gain' # Improves importance visualization
)

# Training
lgb_model.fit(X_train_scaled, y_train)

# Probability predictions
y_probs_lgb = lgb_model.predict_proba(X_test_scaled)[:, 1]

# Let's use the same 0.3 Threshold for a fair comparison with XGBoost
y_pred_lgb = (y_probs_lgb > 0.3).astype(int)

print("\n--- LightGBM Report (Threshold 0.3) ---")
print(classification_report(y_test, y_pred_lgb))

!pip install shap

import shap

# 1. Creating the explainer for XGBoost
explainer = shap.TreeExplainer(xgb_model)

# 2. Calculating SHAP values for a test sample (SHAP takes a while, so we take 500 samples)
X_test_sample = pd.DataFrame(X_test_scaled, columns=X.columns).sample(500, random_state=42)
shap_values = explainer.shap_values(X_test_sample)

# 3. Summary Plot
shap.summary_plot(shap_values, X_test_sample)

